{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.replace('?', ' ? ').replace('!', ' ! ').replace(':', '').replace('(', '').replace(')', '').replace('.', ' . ')\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. The files are all English → Other Language, so if we\n",
    "want to translate from Other Language → English I added the ``reverse``\n",
    "flag to reverse the pairs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_occurrences(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter == ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs():\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    f = open(\"./real/data/ru.conversations.txt\", \"r\")\n",
    "    data = f.read()\n",
    "    \n",
    "    strings = re.split(r'\\n\\n', data)    \n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for dialog in strings:\n",
    "        new_str_pos = find_occurrences(dialog, '\\n')\n",
    "        if len(new_str_pos) > 0:\n",
    "            question = dialog[:new_str_pos[0]]\n",
    "            \n",
    "            \n",
    "            if len(new_str_pos) > 1:\n",
    "                answer = dialog[new_str_pos[0]+1:new_str_pos[1]]\n",
    "            else:\n",
    "                answer = dialog[new_str_pos[0]+1:]\n",
    "            \n",
    "            question = normalizeString(question[2:])\n",
    "            answer = normalizeString(answer[2:])\n",
    "            \n",
    "            pairs.append([question, answer])\n",
    "            \n",
    "\n",
    "    input_lang = Lang('question')\n",
    "    output_lang = Lang('answer')\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we'll trim the data set to only relatively short and\n",
    "simple sentences. Here the maximum length is 10 words (that includes\n",
    "ending punctuation) and we're filtering to sentences that translate to\n",
    "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
    "earlier).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 7\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 84920 sentence pairs\n",
      "Trimmed to 41992 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "question 22786\n",
      "answer 21388\n",
      "['это не материя .', 'а что тогда ?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs()\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('rus', 'rus', True)\n",
    "print(random.choice(pairs))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
    "black cat\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "^^^^^^^^^^^^^^\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we'll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Decoder\n",
    "^^^^^^^^^^^^^^^^^\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    ".. figure:: https://i.imgur.com/1152PYf.png\n",
    "   :alt:\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
    "  limitation by using a relative position approach. Read about \"local\n",
    "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
    "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    ".. Note::\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 29s (- 1091m 53s) (1000 0%) 5.6290\n",
      "6m 15s (- 619m 51s) (2000 1%) 5.3510\n",
      "6m 57s (- 456m 52s) (3000 1%) 5.2599\n",
      "7m 44s (- 379m 20s) (4000 2%) 5.2696\n",
      "8m 26s (- 329m 20s) (5000 2%) 5.3604\n",
      "9m 13s (- 298m 22s) (6000 3%) 5.2685\n",
      "9m 57s (- 274m 25s) (7000 3%) 5.2686\n",
      "10m 43s (- 257m 20s) (8000 4%) 5.2101\n",
      "11m 28s (- 243m 22s) (9000 4%) 5.2109\n",
      "12m 13s (- 232m 14s) (10000 5%) 5.1554\n",
      "12m 44s (- 219m 0s) (11000 5%) 5.2233\n",
      "13m 14s (- 207m 29s) (12000 6%) 5.1478\n",
      "13m 45s (- 197m 50s) (13000 6%) 5.0341\n",
      "14m 14s (- 189m 14s) (14000 7%) 5.1555\n",
      "14m 46s (- 182m 8s) (15000 7%) 5.0426\n",
      "15m 14s (- 175m 18s) (16000 8%) 5.0992\n",
      "15m 47s (- 169m 56s) (17000 8%) 5.1686\n",
      "16m 15s (- 164m 25s) (18000 9%) 4.9764\n",
      "16m 47s (- 160m 1s) (19000 9%) 4.9390\n",
      "17m 17s (- 155m 33s) (20000 10%) 5.0568\n",
      "17m 48s (- 151m 51s) (21000 10%) 5.0491\n",
      "18m 18s (- 148m 9s) (22000 11%) 4.9537\n",
      "18m 49s (- 144m 50s) (23000 11%) 5.0213\n",
      "19m 19s (- 141m 40s) (24000 12%) 5.0420\n",
      "19m 48s (- 138m 41s) (25000 12%) 4.8787\n",
      "20m 19s (- 135m 58s) (26000 13%) 5.0011\n",
      "20m 48s (- 133m 19s) (27000 13%) 4.9249\n",
      "21m 18s (- 130m 51s) (28000 14%) 4.9195\n",
      "21m 48s (- 128m 33s) (29000 14%) 4.9406\n",
      "22m 18s (- 126m 23s) (30000 15%) 5.0258\n",
      "22m 49s (- 124m 27s) (31000 15%) 4.9821\n",
      "23m 19s (- 122m 26s) (32000 16%) 5.0055\n",
      "23m 50s (- 120m 41s) (33000 16%) 4.9306\n",
      "24m 19s (- 118m 45s) (34000 17%) 4.9352\n",
      "24m 51s (- 117m 12s) (35000 17%) 4.9072\n",
      "25m 20s (- 115m 26s) (36000 18%) 4.8987\n",
      "25m 52s (- 114m 1s) (37000 18%) 5.0296\n",
      "26m 20s (- 112m 19s) (38000 19%) 4.9096\n",
      "26m 52s (- 110m 57s) (39000 19%) 4.8857\n",
      "27m 21s (- 109m 24s) (40000 20%) 4.8630\n",
      "27m 52s (- 108m 4s) (41000 20%) 4.8946\n",
      "28m 22s (- 106m 43s) (42000 21%) 4.8082\n",
      "28m 52s (- 105m 26s) (43000 21%) 4.8100\n",
      "29m 24s (- 104m 14s) (44000 22%) 4.8694\n",
      "29m 53s (- 102m 56s) (45000 22%) 4.8756\n",
      "30m 25s (- 101m 51s) (46000 23%) 4.8798\n",
      "30m 52s (- 100m 28s) (47000 23%) 4.9980\n",
      "31m 24s (- 99m 27s) (48000 24%) 4.8147\n",
      "31m 51s (- 98m 11s) (49000 24%) 4.8008\n",
      "32m 24s (- 97m 12s) (50000 25%) 4.7579\n",
      "32m 51s (- 96m 0s) (51000 25%) 4.8437\n",
      "33m 24s (- 95m 4s) (52000 26%) 4.8843\n",
      "33m 53s (- 93m 59s) (53000 26%) 4.9415\n",
      "34m 24s (- 93m 0s) (54000 27%) 4.8511\n",
      "34m 55s (- 92m 4s) (55000 27%) 4.7172\n",
      "35m 23s (- 91m 0s) (56000 28%) 4.8252\n",
      "35m 56s (- 90m 10s) (57000 28%) 4.8174\n",
      "36m 21s (- 89m 1s) (58000 28%) 4.8275\n",
      "36m 54s (- 88m 12s) (59000 29%) 4.8129\n",
      "37m 20s (- 87m 7s) (60000 30%) 4.8383\n",
      "37m 53s (- 86m 20s) (61000 30%) 4.9169\n",
      "38m 23s (- 85m 26s) (62000 31%) 4.9297\n",
      "38m 50s (- 84m 28s) (63000 31%) 4.6832\n",
      "39m 24s (- 83m 44s) (64000 32%) 4.8837\n",
      "39m 47s (- 82m 38s) (65000 32%) 4.8162\n",
      "40m 21s (- 81m 56s) (66000 33%) 4.9315\n",
      "40m 48s (- 80m 59s) (67000 33%) 4.7713\n",
      "41m 19s (- 80m 13s) (68000 34%) 4.7410\n",
      "41m 50s (- 79m 27s) (69000 34%) 4.9274\n",
      "42m 19s (- 78m 36s) (70000 35%) 4.7918\n",
      "42m 53s (- 77m 55s) (71000 35%) 4.7505\n",
      "43m 22s (- 77m 6s) (72000 36%) 4.7287\n",
      "43m 56s (- 76m 26s) (73000 36%) 4.8245\n",
      "44m 24s (- 75m 36s) (74000 37%) 4.6851\n",
      "44m 59s (- 74m 58s) (75000 37%) 4.8110\n",
      "45m 27s (- 74m 9s) (76000 38%) 4.6577\n",
      "46m 2s (- 73m 32s) (77000 38%) 4.7610\n",
      "46m 31s (- 72m 45s) (78000 39%) 4.7303\n",
      "47m 5s (- 72m 7s) (79000 39%) 4.6815\n",
      "47m 35s (- 71m 22s) (80000 40%) 4.6270\n",
      "48m 8s (- 70m 43s) (81000 40%) 4.6813\n",
      "48m 38s (- 70m 0s) (82000 41%) 4.7505\n",
      "49m 11s (- 69m 20s) (83000 41%) 4.7436\n",
      "49m 42s (- 68m 38s) (84000 42%) 4.6716\n",
      "50m 14s (- 67m 58s) (85000 42%) 4.7345\n",
      "50m 45s (- 67m 17s) (86000 43%) 4.7127\n",
      "51m 16s (- 66m 36s) (87000 43%) 4.6644\n",
      "51m 47s (- 65m 55s) (88000 44%) 4.7605\n",
      "52m 18s (- 65m 14s) (89000 44%) 4.7829\n",
      "52m 50s (- 64m 34s) (90000 45%) 4.6969\n",
      "53m 21s (- 63m 54s) (91000 45%) 4.7887\n",
      "53m 53s (- 63m 15s) (92000 46%) 4.6674\n",
      "54m 24s (- 62m 36s) (93000 46%) 4.7472\n",
      "54m 55s (- 61m 56s) (94000 47%) 4.6855\n",
      "55m 27s (- 61m 18s) (95000 47%) 4.7044\n",
      "55m 58s (- 60m 38s) (96000 48%) 4.7370\n",
      "56m 31s (- 60m 0s) (97000 48%) 4.6404\n",
      "57m 2s (- 59m 22s) (98000 49%) 4.7616\n",
      "57m 34s (- 58m 44s) (99000 49%) 4.6234\n",
      "58m 5s (- 58m 5s) (100000 50%) 4.5950\n",
      "58m 37s (- 57m 27s) (101000 50%) 4.6799\n",
      "59m 8s (- 56m 49s) (102000 51%) 4.6538\n",
      "59m 41s (- 56m 12s) (103000 51%) 4.6404\n",
      "60m 12s (- 55m 34s) (104000 52%) 4.6406\n",
      "60m 48s (- 55m 0s) (105000 52%) 4.6106\n",
      "61m 32s (- 54m 34s) (106000 53%) 4.6413\n",
      "62m 18s (- 54m 9s) (107000 53%) 4.7274\n",
      "63m 3s (- 53m 43s) (108000 54%) 4.5986\n",
      "63m 48s (- 53m 16s) (109000 54%) 4.5932\n",
      "64m 33s (- 52m 49s) (110000 55%) 4.6959\n",
      "65m 19s (- 52m 22s) (111000 55%) 4.6237\n",
      "66m 4s (- 51m 54s) (112000 56%) 4.7173\n",
      "66m 48s (- 51m 26s) (113000 56%) 4.6606\n",
      "67m 33s (- 50m 57s) (114000 56%) 4.6477\n",
      "68m 17s (- 50m 28s) (115000 57%) 4.5325\n",
      "69m 2s (- 49m 59s) (116000 57%) 4.5692\n",
      "69m 47s (- 49m 30s) (117000 58%) 4.5959\n",
      "70m 33s (- 49m 1s) (118000 59%) 4.5857\n",
      "71m 16s (- 48m 31s) (119000 59%) 4.6087\n",
      "72m 3s (- 48m 2s) (120000 60%) 4.5961\n",
      "72m 46s (- 47m 30s) (121000 60%) 4.5465\n",
      "73m 33s (- 47m 1s) (122000 61%) 4.6083\n",
      "74m 18s (- 46m 30s) (123000 61%) 4.6939\n",
      "75m 4s (- 46m 0s) (124000 62%) 4.5150\n",
      "75m 48s (- 45m 29s) (125000 62%) 4.4878\n",
      "76m 34s (- 44m 58s) (126000 63%) 4.5996\n",
      "77m 19s (- 44m 26s) (127000 63%) 4.6042\n",
      "78m 4s (- 43m 55s) (128000 64%) 4.5650\n",
      "78m 48s (- 43m 22s) (129000 64%) 4.5142\n",
      "79m 33s (- 42m 50s) (130000 65%) 4.5360\n",
      "80m 19s (- 42m 18s) (131000 65%) 4.5586\n",
      "81m 4s (- 41m 46s) (132000 66%) 4.4560\n",
      "81m 49s (- 41m 13s) (133000 66%) 4.5377\n",
      "82m 33s (- 40m 39s) (134000 67%) 4.5046\n",
      "83m 17s (- 40m 6s) (135000 67%) 4.5384\n",
      "84m 2s (- 39m 33s) (136000 68%) 4.4952\n",
      "84m 47s (- 38m 59s) (137000 68%) 4.5087\n",
      "85m 32s (- 38m 25s) (138000 69%) 4.5913\n",
      "86m 17s (- 37m 52s) (139000 69%) 4.5701\n",
      "87m 2s (- 37m 18s) (140000 70%) 4.5853\n",
      "87m 48s (- 36m 44s) (141000 70%) 4.5618\n",
      "88m 32s (- 36m 10s) (142000 71%) 4.5397\n",
      "89m 18s (- 35m 35s) (143000 71%) 4.4434\n",
      "90m 2s (- 35m 0s) (144000 72%) 4.5454\n",
      "90m 49s (- 34m 26s) (145000 72%) 4.4488\n",
      "91m 31s (- 33m 51s) (146000 73%) 4.5111\n",
      "92m 18s (- 33m 17s) (147000 73%) 4.4989\n",
      "93m 2s (- 32m 41s) (148000 74%) 4.4690\n",
      "93m 48s (- 32m 6s) (149000 74%) 4.3370\n",
      "94m 31s (- 31m 30s) (150000 75%) 4.4174\n",
      "95m 17s (- 30m 55s) (151000 75%) 4.4131\n",
      "96m 2s (- 30m 19s) (152000 76%) 4.5606\n",
      "96m 46s (- 29m 43s) (153000 76%) 4.4743\n",
      "97m 31s (- 29m 7s) (154000 77%) 4.4513\n",
      "98m 17s (- 28m 32s) (155000 77%) 4.4734\n",
      "99m 2s (- 27m 56s) (156000 78%) 4.5220\n",
      "99m 46s (- 27m 19s) (157000 78%) 4.4009\n",
      "100m 32s (- 26m 43s) (158000 79%) 4.4430\n",
      "101m 17s (- 26m 7s) (159000 79%) 4.5296\n",
      "102m 2s (- 25m 30s) (160000 80%) 4.3957\n",
      "102m 47s (- 24m 53s) (161000 80%) 4.4608\n",
      "103m 31s (- 24m 17s) (162000 81%) 4.3542\n",
      "104m 17s (- 23m 40s) (163000 81%) 4.5268\n",
      "105m 2s (- 23m 3s) (164000 82%) 4.3890\n",
      "105m 48s (- 22m 26s) (165000 82%) 4.5146\n",
      "106m 33s (- 21m 49s) (166000 83%) 4.4358\n",
      "107m 19s (- 21m 12s) (167000 83%) 4.3238\n",
      "108m 4s (- 20m 35s) (168000 84%) 4.3859\n",
      "108m 49s (- 19m 57s) (169000 84%) 4.4505\n",
      "109m 35s (- 19m 20s) (170000 85%) 4.3774\n",
      "110m 20s (- 18m 42s) (171000 85%) 4.3615\n",
      "111m 7s (- 18m 5s) (172000 86%) 4.3720\n",
      "111m 51s (- 17m 27s) (173000 86%) 4.4091\n",
      "112m 38s (- 16m 49s) (174000 87%) 4.3636\n",
      "113m 22s (- 16m 11s) (175000 87%) 4.3627\n",
      "114m 9s (- 15m 34s) (176000 88%) 4.2741\n",
      "114m 53s (- 14m 55s) (177000 88%) 4.4492\n",
      "115m 40s (- 14m 17s) (178000 89%) 4.2928\n",
      "116m 25s (- 13m 39s) (179000 89%) 4.5074\n",
      "117m 11s (- 13m 1s) (180000 90%) 4.3960\n",
      "117m 56s (- 12m 22s) (181000 90%) 4.3343\n",
      "118m 42s (- 11m 44s) (182000 91%) 4.3163\n",
      "119m 26s (- 11m 5s) (183000 91%) 4.2507\n",
      "120m 12s (- 10m 27s) (184000 92%) 4.3181\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 500000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), \"./models/encoder1_rus_conv_hs_128_norm\")\n",
    "torch.save(attn_decoder1.state_dict(), \"./models/attn_decoder1_rus_conv_hs_128_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Что ты ему дал?\n",
      "= Ты  знаешь  что.\n",
      "< Ничего. <EOS>\n",
      "\n",
      "> Из-за его поведения.\n",
      "= Поведения?\n",
      "< А  <EOS>\n",
      "\n",
      "> Генерал  Коуэлл! Вы меня слышите?\n",
      "= Я  вас  слышу.\n",
      "< Да. <EOS>\n",
      "\n",
      "> Что это? Интрижка с прислугой?\n",
      "= Нет! А кто вы, собственно, есть?\n",
      "< Ничего. <EOS>\n",
      "\n",
      "> Елена.\n",
      "= Елена?\n",
      "< Что  <EOS>\n",
      "\n",
      "> Глория, где он?\n",
      "= Ты заткнешься?\n",
      "< У меня не <EOS>\n",
      "\n",
      "> Жан-Клод?\n",
      "= Да, это я, ma petite.\n",
      "< А  <EOS>\n",
      "\n",
      "> Мы скоро вернемся.\n",
      "= Отлично.\n",
      "< Да. <EOS>\n",
      "\n",
      "> Она родит мне могучее дитя.\n",
      "= Кто? У тебя есть жена?\n",
      "< А как ты говоришь? <EOS>\n",
      "\n",
      "> Не  открывается.\n",
      "= Ой!\n",
      "< Что ты что это <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Что  <EOS>\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"\"\n",
    "output_words, attentions = evaluate(encoder1, attn_decoder1, input_sentence)\n",
    "output_sentence = ' '.join(output_words)\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "---------------------\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix, with the columns being input steps and rows being\n",
    "output steps:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17ed17278>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 672x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"дела?\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes\n",
    "and labels:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'елка'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-293919be6872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"елка .\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"воу .\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-293919be6872>\u001b[0m in \u001b[0;36mevaluateAndShowAttention\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     output_words, attentions = evaluate(\n\u001b[0;32m---> 22\u001b[0;31m         encoder1, attn_decoder1, input_sentence)\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-881d27a56888>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bb69e91c7422>\u001b[0m in \u001b[0;36mtensorFromSentence\u001b[0;34m(lang, sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEOS_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bb69e91c7422>\u001b[0m in \u001b[0;36mindexesFromSentence\u001b[0;34m(lang, sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bb69e91c7422>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'елка'"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"елка .\")\n",
    "\n",
    "evaluateAndShowAttention(\"воу .\")\n",
    "\n",
    "evaluateAndShowAttention(\"привет .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Try with a different dataset\n",
    "\n",
    "   -  Another language pair\n",
    "   -  Human → Machine (e.g. IOT commands)\n",
    "   -  Chat → Response\n",
    "   -  Question → Answer\n",
    "\n",
    "-  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
    "   GloVe\n",
    "-  Try with more layers, more hidden units, and more sentences. Compare\n",
    "   the training time and results.\n",
    "-  If you use a translation file where pairs have two of the same phrase\n",
    "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
    "   this:\n",
    "\n",
    "   -  Train as an autoencoder\n",
    "   -  Save only the Encoder network\n",
    "   -  Train a new Decoder for translation from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('seq2seq_200000.pk', 'wb') as f:\n",
    "    data = {\n",
    "        'questions': input_lang,\n",
    "        'answers': output_lang,\n",
    "        'encoder': encoder1,\n",
    "        'decoder': attn_decoder1\n",
    "    }\n",
    "    pickle.dump(data, d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    if (':' in s[:5]):\n",
    "        s = s[:s.find(':')] + s[s.find(':') + 1:]\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. The files are all English → Other Language, so if we\n",
    "want to translate from Other Language → English I added the ``reverse``\n",
    "flag to reverse the pairs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_occurrences(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter == ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs():\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    f = open(\"./real/data/ru.conversations.txt\", \"r\")\n",
    "    data = f.read()\n",
    "    \n",
    "    strings = re.split(r'\\n\\n', data)    \n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for dialog in strings:\n",
    "        new_str_pos = find_occurrences(dialog, '\\n')\n",
    "        if len(new_str_pos) > 0:\n",
    "            question = dialog[:new_str_pos[0]]\n",
    "            \n",
    "            \n",
    "            if len(new_str_pos) > 1:\n",
    "                answer = dialog[new_str_pos[0]+1:new_str_pos[1]]\n",
    "            else:\n",
    "                answer = dialog[new_str_pos[0]+1:]\n",
    "            \n",
    "            question = normalizeString(question[2:])\n",
    "            answer = normalizeString(answer[2:])\n",
    "            \n",
    "            pairs.append([question, answer])\n",
    "            \n",
    "\n",
    "    input_lang = Lang('question')\n",
    "    output_lang = Lang('answer')\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we'll trim the data set to only relatively short and\n",
    "simple sentences. Here the maximum length is 10 words (that includes\n",
    "ending punctuation) and we're filtering to sentences that translate to\n",
    "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
    "earlier).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 5\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 84920 sentence pairs\n",
      "Trimmed to 35451 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "question 24405\n",
      "answer 23156\n",
      "['господин христофор гонзо?', 'простите.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs()\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('rus', 'rus', True)\n",
    "print(random.choice(pairs))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
    "black cat\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "^^^^^^^^^^^^^^\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we'll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Decoder\n",
    "^^^^^^^^^^^^^^^^^\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    ".. figure:: https://i.imgur.com/1152PYf.png\n",
    "   :alt:\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
    "  limitation by using a relative position approach. Read about \"local\n",
    "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
    "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    ".. Note::\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m 51s (- 4330m 35s) (500 0%) 5.6875\n",
      "11m 2s (- 2195m 56s) (1000 0%) 5.3822\n",
      "11m 12s (- 1482m 35s) (1500 0%) 5.2385\n",
      "11m 22s (- 1125m 23s) (2000 1%) 5.2518\n",
      "11m 33s (- 912m 57s) (2500 1%) 5.2320\n",
      "11m 44s (- 771m 6s) (3000 1%) 5.2590\n",
      "11m 55s (- 669m 9s) (3500 1%) 5.1813\n",
      "12m 5s (- 592m 8s) (4000 2%) 5.1614\n",
      "12m 16s (- 533m 4s) (4500 2%) 5.0787\n",
      "12m 27s (- 485m 41s) (5000 2%) 4.9814\n",
      "12m 38s (- 446m 49s) (5500 2%) 4.8958\n",
      "12m 47s (- 413m 51s) (6000 3%) 5.0299\n",
      "12m 57s (- 385m 56s) (6500 3%) 4.9941\n",
      "13m 8s (- 362m 31s) (7000 3%) 5.1510\n",
      "13m 20s (- 342m 19s) (7500 3%) 5.2198\n",
      "13m 30s (- 324m 10s) (8000 4%) 5.0603\n",
      "13m 40s (- 308m 11s) (8500 4%) 5.0511\n",
      "13m 51s (- 294m 9s) (9000 4%) 4.9363\n",
      "14m 2s (- 281m 36s) (9500 4%) 4.9626\n",
      "14m 13s (- 270m 22s) (10000 5%) 4.9759\n",
      "14m 23s (- 259m 43s) (10500 5%) 4.9490\n",
      "14m 33s (- 250m 10s) (11000 5%) 4.9617\n",
      "14m 44s (- 241m 41s) (11500 5%) 4.8801\n",
      "14m 55s (- 233m 52s) (12000 6%) 4.8967\n",
      "15m 5s (- 226m 17s) (12500 6%) 4.7768\n",
      "15m 16s (- 219m 39s) (13000 6%) 4.9420\n",
      "15m 27s (- 213m 30s) (13500 6%) 4.9375\n",
      "15m 38s (- 207m 46s) (14000 7%) 4.8625\n",
      "15m 49s (- 202m 26s) (14500 7%) 4.8223\n",
      "15m 58s (- 197m 6s) (15000 7%) 4.7632\n",
      "16m 9s (- 192m 16s) (15500 7%) 4.8785\n",
      "16m 20s (- 187m 55s) (16000 8%) 4.9887\n",
      "16m 30s (- 183m 40s) (16500 8%) 4.9778\n",
      "16m 40s (- 179m 31s) (17000 8%) 4.8180\n",
      "16m 51s (- 175m 51s) (17500 8%) 4.9396\n",
      "17m 2s (- 172m 23s) (18000 9%) 4.8743\n",
      "17m 13s (- 169m 3s) (18500 9%) 4.9300\n",
      "17m 24s (- 165m 54s) (19000 9%) 4.9858\n",
      "17m 34s (- 162m 39s) (19500 9%) 4.8291\n",
      "17m 44s (- 159m 43s) (20000 10%) 4.8292\n",
      "17m 55s (- 157m 0s) (20500 10%) 4.8816\n",
      "18m 6s (- 154m 17s) (21000 10%) 4.8371\n",
      "18m 15s (- 151m 37s) (21500 10%) 4.7663\n",
      "18m 27s (- 149m 17s) (22000 11%) 4.8784\n",
      "18m 38s (- 147m 1s) (22500 11%) 4.6707\n",
      "18m 49s (- 144m 49s) (23000 11%) 4.6882\n",
      "19m 0s (- 142m 42s) (23500 11%) 4.9250\n",
      "19m 9s (- 140m 28s) (24000 12%) 4.7147\n",
      "19m 20s (- 138m 29s) (24500 12%) 4.7735\n",
      "19m 31s (- 136m 38s) (25000 12%) 4.8051\n",
      "19m 41s (- 134m 43s) (25500 12%) 4.7533\n",
      "19m 51s (- 132m 53s) (26000 13%) 4.9030\n",
      "20m 2s (- 131m 12s) (26500 13%) 4.8315\n",
      "20m 13s (- 129m 35s) (27000 13%) 4.7591\n",
      "20m 24s (- 128m 1s) (27500 13%) 4.6884\n",
      "20m 35s (- 126m 27s) (28000 14%) 4.9134\n",
      "20m 44s (- 124m 49s) (28500 14%) 4.9603\n",
      "20m 55s (- 123m 23s) (29000 14%) 4.8078\n",
      "21m 6s (- 122m 1s) (29500 14%) 4.7935\n",
      "21m 16s (- 120m 33s) (30000 15%) 4.7007\n",
      "21m 26s (- 119m 10s) (30500 15%) 4.7557\n",
      "21m 37s (- 117m 56s) (31000 15%) 4.8510\n",
      "21m 48s (- 116m 42s) (31500 15%) 4.8012\n",
      "22m 0s (- 115m 31s) (32000 16%) 4.7011\n",
      "22m 10s (- 114m 17s) (32500 16%) 4.8062\n",
      "22m 20s (- 113m 2s) (33000 16%) 4.7544\n",
      "22m 31s (- 111m 55s) (33500 16%) 4.7425\n",
      "22m 42s (- 110m 50s) (34000 17%) 4.7212\n",
      "22m 51s (- 109m 40s) (34500 17%) 4.7938\n",
      "23m 2s (- 108m 36s) (35000 17%) 4.8171\n",
      "23m 13s (- 107m 36s) (35500 17%) 4.6736\n",
      "23m 24s (- 106m 38s) (36000 18%) 4.8121\n",
      "23m 35s (- 105m 41s) (36500 18%) 4.9223\n",
      "23m 45s (- 104m 40s) (37000 18%) 4.7582\n",
      "23m 55s (- 103m 41s) (37500 18%) 4.8777\n",
      "24m 6s (- 102m 47s) (38000 19%) 4.7996\n",
      "24m 17s (- 101m 55s) (38500 19%) 4.8684\n",
      "24m 27s (- 100m 58s) (39000 19%) 4.6289\n",
      "24m 37s (- 100m 5s) (39500 19%) 4.7021\n",
      "24m 48s (- 99m 15s) (40000 20%) 4.6422\n",
      "24m 59s (- 98m 27s) (40500 20%) 4.6966\n",
      "25m 10s (- 97m 39s) (41000 20%) 4.7491\n",
      "25m 20s (- 96m 49s) (41500 20%) 4.7565\n",
      "25m 31s (- 95m 59s) (42000 21%) 4.7408\n",
      "25m 42s (- 95m 15s) (42500 21%) 4.7468\n",
      "25m 53s (- 94m 30s) (43000 21%) 4.8418\n",
      "26m 2s (- 93m 41s) (43500 21%) 4.6447\n",
      "26m 13s (- 92m 58s) (44000 22%) 4.7601\n",
      "26m 24s (- 92m 16s) (44500 22%) 4.6835\n",
      "26m 35s (- 91m 35s) (45000 22%) 4.5514\n",
      "26m 46s (- 90m 56s) (45500 22%) 4.8239\n",
      "26m 56s (- 90m 11s) (46000 23%) 4.7240\n",
      "27m 6s (- 89m 30s) (46500 23%) 4.7646\n",
      "27m 17s (- 88m 51s) (47000 23%) 4.7315\n",
      "27m 28s (- 88m 12s) (47500 23%) 4.8072\n",
      "27m 38s (- 87m 30s) (48000 24%) 4.7015\n",
      "27m 49s (- 86m 54s) (48500 24%) 4.6113\n",
      "28m 0s (- 86m 19s) (49000 24%) 4.5842\n",
      "28m 11s (- 85m 44s) (49500 24%) 4.6377\n",
      "28m 22s (- 85m 8s) (50000 25%) 4.7669\n",
      "28m 32s (- 84m 28s) (50500 25%) 4.6254\n",
      "28m 42s (- 83m 53s) (51000 25%) 4.7823\n",
      "28m 53s (- 83m 19s) (51500 25%) 4.6511\n",
      "29m 4s (- 82m 43s) (52000 26%) 4.6098\n",
      "29m 14s (- 82m 8s) (52500 26%) 4.6634\n",
      "29m 25s (- 81m 36s) (53000 26%) 4.7134\n",
      "29m 36s (- 81m 4s) (53500 26%) 4.6915\n",
      "29m 47s (- 80m 32s) (54000 27%) 4.5652\n",
      "29m 57s (- 79m 59s) (54500 27%) 4.5490\n",
      "30m 7s (- 79m 25s) (55000 27%) 4.8164\n",
      "30m 18s (- 78m 54s) (55500 27%) 4.5922\n",
      "30m 29s (- 78m 25s) (56000 28%) 4.7910\n",
      "30m 39s (- 77m 52s) (56500 28%) 4.7696\n",
      "30m 49s (- 77m 21s) (57000 28%) 4.4954\n",
      "31m 1s (- 76m 52s) (57500 28%) 4.5335\n",
      "31m 12s (- 76m 24s) (58000 28%) 4.7144\n",
      "31m 23s (- 75m 56s) (58500 29%) 4.7740\n",
      "31m 33s (- 75m 26s) (59000 29%) 4.5486\n",
      "31m 43s (- 74m 54s) (59500 29%) 4.5275\n",
      "31m 54s (- 74m 27s) (60000 30%) 4.6493\n",
      "32m 5s (- 74m 0s) (60500 30%) 4.5805\n",
      "32m 15s (- 73m 29s) (61000 30%) 4.6418\n",
      "32m 25s (- 73m 2s) (61500 30%) 4.6606\n",
      "32m 37s (- 72m 36s) (62000 31%) 4.6505\n",
      "32m 48s (- 72m 10s) (62500 31%) 4.5847\n",
      "32m 59s (- 71m 44s) (63000 31%) 4.5836\n",
      "33m 9s (- 71m 16s) (63500 31%) 4.7362\n",
      "33m 19s (- 70m 47s) (64000 32%) 4.4763\n",
      "33m 30s (- 70m 22s) (64500 32%) 4.6993\n",
      "33m 40s (- 69m 57s) (65000 32%) 4.6444\n",
      "33m 50s (- 69m 29s) (65500 32%) 4.5604\n",
      "34m 1s (- 69m 5s) (66000 33%) 4.5747\n",
      "34m 12s (- 68m 40s) (66500 33%) 4.5927\n",
      "34m 23s (- 68m 16s) (67000 33%) 4.5841\n",
      "34m 35s (- 67m 53s) (67500 33%) 4.8040\n",
      "34m 44s (- 67m 26s) (68000 34%) 4.7216\n",
      "34m 55s (- 67m 1s) (68500 34%) 4.6398\n",
      "35m 6s (- 66m 38s) (69000 34%) 4.4974\n",
      "35m 16s (- 66m 14s) (69500 34%) 4.6055\n",
      "35m 26s (- 65m 49s) (70000 35%) 4.4999\n",
      "35m 37s (- 65m 26s) (70500 35%) 4.5023\n",
      "35m 48s (- 65m 4s) (71000 35%) 4.6613\n",
      "36m 0s (- 64m 42s) (71500 35%) 4.4039\n",
      "36m 11s (- 64m 19s) (72000 36%) 4.5017\n",
      "36m 20s (- 63m 54s) (72500 36%) 4.6386\n",
      "36m 30s (- 63m 31s) (73000 36%) 4.6454\n",
      "36m 42s (- 63m 10s) (73500 36%) 4.4876\n",
      "36m 52s (- 62m 46s) (74000 37%) 4.5817\n",
      "37m 2s (- 62m 23s) (74500 37%) 4.5402\n",
      "37m 13s (- 62m 2s) (75000 37%) 4.6447\n",
      "37m 24s (- 61m 41s) (75500 37%) 4.4652\n",
      "37m 35s (- 61m 20s) (76000 38%) 4.6301\n",
      "37m 46s (- 60m 58s) (76500 38%) 4.6024\n",
      "37m 55s (- 60m 35s) (77000 38%) 4.6812\n",
      "38m 6s (- 60m 14s) (77500 38%) 4.6304\n",
      "38m 18s (- 59m 54s) (78000 39%) 4.6673\n",
      "38m 28s (- 59m 32s) (78500 39%) 4.5041\n",
      "38m 38s (- 59m 10s) (79000 39%) 4.6706\n",
      "38m 49s (- 58m 50s) (79500 39%) 4.5514\n",
      "39m 0s (- 58m 31s) (80000 40%) 4.4427\n",
      "39m 11s (- 58m 11s) (80500 40%) 4.5755\n",
      "39m 22s (- 57m 50s) (81000 40%) 4.5570\n",
      "39m 32s (- 57m 29s) (81500 40%) 4.6188\n",
      "39m 43s (- 57m 9s) (82000 41%) 4.5920\n",
      "39m 54s (- 56m 50s) (82500 41%) 4.5003\n",
      "40m 4s (- 56m 28s) (83000 41%) 4.5481\n",
      "40m 14s (- 56m 8s) (83500 41%) 4.6267\n",
      "40m 25s (- 55m 50s) (84000 42%) 4.5589\n",
      "40m 37s (- 55m 31s) (84500 42%) 4.5328\n",
      "40m 48s (- 55m 12s) (85000 42%) 4.5619\n",
      "40m 58s (- 54m 52s) (85500 42%) 4.5205\n",
      "41m 8s (- 54m 32s) (86000 43%) 4.5193\n",
      "41m 19s (- 54m 13s) (86500 43%) 4.4913\n",
      "41m 30s (- 53m 54s) (87000 43%) 4.6089\n",
      "41m 40s (- 53m 34s) (87500 43%) 4.6400\n",
      "41m 51s (- 53m 16s) (88000 44%) 4.5919\n",
      "42m 2s (- 52m 58s) (88500 44%) 4.5766\n",
      "42m 13s (- 52m 40s) (89000 44%) 4.5743\n",
      "42m 24s (- 52m 22s) (89500 44%) 4.4118\n",
      "42m 34s (- 52m 2s) (90000 45%) 4.6515\n",
      "42m 44s (- 51m 43s) (90500 45%) 4.4138\n",
      "42m 56s (- 51m 25s) (91000 45%) 4.5746\n",
      "43m 6s (- 51m 6s) (91500 45%) 4.6161\n",
      "43m 15s (- 50m 47s) (92000 46%) 4.4086\n",
      "43m 27s (- 50m 29s) (92500 46%) 4.5221\n",
      "43m 38s (- 50m 12s) (93000 46%) 4.5099\n",
      "43m 49s (- 49m 55s) (93500 46%) 4.5060\n",
      "44m 0s (- 49m 37s) (94000 47%) 4.6321\n",
      "44m 10s (- 49m 18s) (94500 47%) 4.5541\n",
      "44m 20s (- 49m 0s) (95000 47%) 4.5231\n",
      "44m 31s (- 48m 43s) (95500 47%) 4.4539\n",
      "44m 42s (- 48m 25s) (96000 48%) 4.4438\n",
      "44m 51s (- 48m 7s) (96500 48%) 4.3786\n",
      "45m 2s (- 47m 50s) (97000 48%) 4.3917\n",
      "45m 14s (- 47m 33s) (97500 48%) 4.5489\n",
      "45m 25s (- 47m 16s) (98000 49%) 4.6154\n",
      "45m 35s (- 46m 59s) (98500 49%) 4.5082\n",
      "45m 45s (- 46m 41s) (99000 49%) 4.4313\n",
      "45m 56s (- 46m 24s) (99500 49%) 4.4586\n",
      "46m 7s (- 46m 7s) (100000 50%) 4.4244\n",
      "46m 18s (- 45m 50s) (100500 50%) 4.4631\n",
      "46m 27s (- 45m 32s) (101000 50%) 4.5599\n",
      "46m 38s (- 45m 16s) (101500 50%) 4.4698\n",
      "46m 50s (- 44m 59s) (102000 51%) 4.5216\n",
      "47m 1s (- 44m 43s) (102500 51%) 4.5201\n",
      "47m 11s (- 44m 26s) (103000 51%) 4.6437\n",
      "47m 21s (- 44m 9s) (103500 51%) 4.5338\n",
      "47m 32s (- 43m 53s) (104000 52%) 4.4022\n",
      "47m 44s (- 43m 37s) (104500 52%) 4.5416\n",
      "47m 54s (- 43m 20s) (105000 52%) 4.4853\n",
      "48m 4s (- 43m 3s) (105500 52%) 4.4733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48m 15s (- 42m 47s) (106000 53%) 4.3534\n",
      "48m 27s (- 42m 32s) (106500 53%) 4.5200\n",
      "48m 38s (- 42m 16s) (107000 53%) 4.4736\n",
      "48m 48s (- 41m 59s) (107500 53%) 4.5563\n",
      "48m 58s (- 41m 42s) (108000 54%) 4.3597\n",
      "49m 9s (- 41m 27s) (108500 54%) 4.4857\n",
      "49m 20s (- 41m 11s) (109000 54%) 4.4949\n",
      "49m 30s (- 40m 54s) (109500 54%) 4.3928\n",
      "49m 40s (- 40m 38s) (110000 55%) 4.5561\n",
      "49m 51s (- 40m 23s) (110500 55%) 4.4298\n",
      "50m 2s (- 40m 7s) (111000 55%) 4.4727\n",
      "50m 13s (- 39m 52s) (111500 55%) 4.5456\n",
      "50m 23s (- 39m 35s) (112000 56%) 4.4416\n",
      "50m 33s (- 39m 19s) (112500 56%) 4.3504\n",
      "50m 45s (- 39m 4s) (113000 56%) 4.5292\n",
      "50m 56s (- 38m 49s) (113500 56%) 4.3582\n",
      "51m 6s (- 38m 33s) (114000 56%) 4.5505\n",
      "51m 17s (- 38m 17s) (114500 57%) 4.4312\n",
      "51m 28s (- 38m 2s) (115000 57%) 4.4744\n",
      "51m 39s (- 37m 47s) (115500 57%) 4.3882\n",
      "51m 50s (- 37m 32s) (116000 57%) 4.4427\n",
      "51m 59s (- 37m 15s) (116500 58%) 4.3590\n",
      "52m 10s (- 37m 0s) (117000 58%) 4.3866\n",
      "52m 21s (- 36m 45s) (117500 58%) 4.4964\n",
      "52m 32s (- 36m 30s) (118000 59%) 4.4629\n",
      "52m 42s (- 36m 15s) (118500 59%) 4.5299\n",
      "52m 53s (- 36m 0s) (119000 59%) 4.3262\n",
      "53m 4s (- 35m 45s) (119500 59%) 4.4546\n",
      "53m 15s (- 35m 30s) (120000 60%) 4.5222\n",
      "53m 26s (- 35m 15s) (120500 60%) 4.3832\n",
      "53m 35s (- 34m 59s) (121000 60%) 4.4232\n",
      "53m 47s (- 34m 44s) (121500 60%) 4.2925\n",
      "53m 58s (- 34m 30s) (122000 61%) 4.3487\n",
      "54m 8s (- 34m 15s) (122500 61%) 4.4090\n",
      "54m 18s (- 33m 59s) (123000 61%) 4.2826\n",
      "54m 29s (- 33m 45s) (123500 61%) 4.2757\n",
      "54m 41s (- 33m 30s) (124000 62%) 4.4726\n",
      "54m 52s (- 33m 16s) (124500 62%) 4.4809\n",
      "55m 2s (- 33m 1s) (125000 62%) 4.3884\n",
      "55m 12s (- 32m 46s) (125500 62%) 4.2737\n",
      "55m 23s (- 32m 32s) (126000 63%) 4.2731\n",
      "55m 35s (- 32m 17s) (126500 63%) 4.4401\n",
      "55m 45s (- 32m 2s) (127000 63%) 4.3597\n",
      "55m 55s (- 31m 47s) (127500 63%) 4.2140\n",
      "56m 6s (- 31m 33s) (128000 64%) 4.2898\n",
      "56m 17s (- 31m 19s) (128500 64%) 4.3912\n",
      "56m 28s (- 31m 5s) (129000 64%) 4.2888\n",
      "56m 38s (- 30m 50s) (129500 64%) 4.3929\n",
      "56m 48s (- 30m 35s) (130000 65%) 4.3660\n",
      "57m 0s (- 30m 21s) (130500 65%) 4.2752\n",
      "57m 11s (- 30m 7s) (131000 65%) 4.3064\n",
      "57m 21s (- 29m 52s) (131500 65%) 4.2863\n",
      "57m 32s (- 29m 38s) (132000 66%) 4.4429\n",
      "57m 43s (- 29m 24s) (132500 66%) 4.2547\n",
      "57m 54s (- 29m 10s) (133000 66%) 4.4046\n",
      "58m 5s (- 28m 56s) (133500 66%) 4.3496\n",
      "58m 14s (- 28m 41s) (134000 67%) 4.4431\n",
      "58m 26s (- 28m 27s) (134500 67%) 4.3373\n",
      "58m 37s (- 28m 13s) (135000 67%) 4.3150\n",
      "58m 47s (- 27m 59s) (135500 67%) 4.2858\n",
      "58m 57s (- 27m 44s) (136000 68%) 4.4574\n",
      "59m 9s (- 27m 31s) (136500 68%) 4.3644\n",
      "59m 20s (- 27m 17s) (137000 68%) 4.2941\n",
      "59m 31s (- 27m 3s) (137500 68%) 4.2788\n",
      "59m 41s (- 26m 49s) (138000 69%) 4.3117\n",
      "59m 51s (- 26m 34s) (138500 69%) 4.1615\n",
      "60m 2s (- 26m 21s) (139000 69%) 4.3325\n",
      "60m 14s (- 26m 7s) (139500 69%) 4.3518\n",
      "60m 24s (- 25m 53s) (140000 70%) 4.2771\n",
      "60m 34s (- 25m 39s) (140500 70%) 4.4135\n",
      "60m 46s (- 25m 25s) (141000 70%) 4.2850\n",
      "60m 57s (- 25m 11s) (141500 70%) 4.1593\n",
      "61m 8s (- 24m 58s) (142000 71%) 4.3240\n",
      "61m 18s (- 24m 44s) (142500 71%) 4.2280\n",
      "61m 28s (- 24m 30s) (143000 71%) 4.3414\n",
      "61m 39s (- 24m 16s) (143500 71%) 4.3860\n",
      "61m 50s (- 24m 2s) (144000 72%) 4.1686\n",
      "61m 59s (- 23m 48s) (144500 72%) 4.1981\n",
      "62m 11s (- 23m 35s) (145000 72%) 4.1943\n",
      "62m 22s (- 23m 21s) (145500 72%) 4.3106\n",
      "62m 33s (- 23m 8s) (146000 73%) 4.2296\n",
      "62m 44s (- 22m 54s) (146500 73%) 4.2417\n",
      "62m 54s (- 22m 40s) (147000 73%) 4.4031\n",
      "63m 4s (- 22m 27s) (147500 73%) 4.1197\n",
      "63m 16s (- 22m 13s) (148000 74%) 4.3198\n",
      "63m 26s (- 22m 0s) (148500 74%) 4.2066\n",
      "63m 36s (- 21m 46s) (149000 74%) 4.3251\n",
      "63m 47s (- 21m 32s) (149500 74%) 4.1968\n",
      "63m 58s (- 21m 19s) (150000 75%) 4.2186\n",
      "64m 10s (- 21m 6s) (150500 75%) 4.3889\n",
      "64m 21s (- 20m 52s) (151000 75%) 4.3101\n",
      "64m 30s (- 20m 39s) (151500 75%) 4.2941\n",
      "64m 41s (- 20m 25s) (152000 76%) 4.0547\n",
      "64m 52s (- 20m 12s) (152500 76%) 4.4006\n",
      "65m 2s (- 19m 58s) (153000 76%) 4.3645\n",
      "65m 13s (- 19m 45s) (153500 76%) 4.2954\n",
      "65m 24s (- 19m 32s) (154000 77%) 4.2435\n",
      "65m 35s (- 19m 19s) (154500 77%) 4.2192\n",
      "65m 47s (- 19m 5s) (155000 77%) 4.2767\n",
      "65m 57s (- 18m 52s) (155500 77%) 4.1829\n",
      "66m 7s (- 18m 38s) (156000 78%) 4.1666\n",
      "66m 18s (- 18m 25s) (156500 78%) 4.2130\n",
      "66m 29s (- 18m 12s) (157000 78%) 4.2836\n",
      "66m 38s (- 17m 59s) (157500 78%) 4.1797\n",
      "66m 50s (- 17m 45s) (158000 79%) 4.3295\n",
      "67m 1s (- 17m 32s) (158500 79%) 4.2583\n",
      "67m 12s (- 17m 19s) (159000 79%) 4.3002\n",
      "67m 24s (- 17m 6s) (159500 79%) 4.1544\n",
      "67m 34s (- 16m 53s) (160000 80%) 4.2562\n",
      "67m 44s (- 16m 40s) (160500 80%) 4.1913\n",
      "67m 55s (- 16m 27s) (161000 80%) 4.1763\n",
      "68m 5s (- 16m 14s) (161500 80%) 4.2511\n",
      "68m 16s (- 16m 0s) (162000 81%) 4.2515\n",
      "68m 27s (- 15m 47s) (162500 81%) 4.1402\n",
      "68m 38s (- 15m 34s) (163000 81%) 4.2298\n",
      "68m 49s (- 15m 21s) (163500 81%) 4.0385\n",
      "69m 1s (- 15m 9s) (164000 82%) 4.2841\n",
      "69m 10s (- 14m 55s) (164500 82%) 4.1797\n",
      "69m 20s (- 14m 42s) (165000 82%) 4.2479\n",
      "69m 32s (- 14m 29s) (165500 82%) 4.2570\n",
      "69m 41s (- 14m 16s) (166000 83%) 4.1244\n",
      "69m 52s (- 14m 3s) (166500 83%) 4.1055\n",
      "70m 4s (- 13m 50s) (167000 83%) 4.1288\n",
      "70m 15s (- 13m 37s) (167500 83%) 4.1626\n",
      "70m 26s (- 13m 25s) (168000 84%) 4.2059\n",
      "70m 37s (- 13m 12s) (168500 84%) 4.1689\n",
      "70m 47s (- 12m 59s) (169000 84%) 4.1975\n",
      "70m 58s (- 12m 46s) (169500 84%) 4.2390\n",
      "71m 9s (- 12m 33s) (170000 85%) 4.2563\n",
      "71m 19s (- 12m 20s) (170500 85%) 4.2645\n",
      "71m 30s (- 12m 7s) (171000 85%) 4.0564\n",
      "71m 42s (- 11m 54s) (171500 85%) 4.1544\n",
      "71m 53s (- 11m 42s) (172000 86%) 4.1049\n",
      "72m 4s (- 11m 29s) (172500 86%) 4.2322\n",
      "72m 15s (- 11m 16s) (173000 86%) 4.0091\n",
      "72m 25s (- 11m 3s) (173500 86%) 4.1871\n",
      "72m 35s (- 10m 50s) (174000 87%) 4.1875\n",
      "72m 45s (- 10m 37s) (174500 87%) 4.2085\n",
      "72m 56s (- 10m 25s) (175000 87%) 4.0589\n",
      "73m 7s (- 10m 12s) (175500 87%) 4.2329\n",
      "73m 18s (- 9m 59s) (176000 88%) 4.1774\n",
      "73m 30s (- 9m 47s) (176500 88%) 4.0961\n",
      "73m 41s (- 9m 34s) (177000 88%) 4.1825\n",
      "73m 52s (- 9m 21s) (177500 88%) 4.2431\n",
      "74m 1s (- 9m 8s) (178000 89%) 4.1601\n",
      "74m 12s (- 8m 56s) (178500 89%) 4.2289\n",
      "74m 22s (- 8m 43s) (179000 89%) 4.1823\n",
      "74m 33s (- 8m 30s) (179500 89%) 4.1108\n",
      "74m 45s (- 8m 18s) (180000 90%) 4.1530\n",
      "74m 56s (- 8m 5s) (180500 90%) 4.1547\n",
      "75m 7s (- 7m 53s) (181000 90%) 4.0654\n",
      "75m 19s (- 7m 40s) (181500 90%) 4.3022\n",
      "75m 29s (- 7m 27s) (182000 91%) 4.1254\n",
      "75m 39s (- 7m 15s) (182500 91%) 4.2261\n",
      "75m 49s (- 7m 2s) (183000 91%) 4.0542\n",
      "75m 59s (- 6m 50s) (183500 91%) 4.1750\n",
      "76m 11s (- 6m 37s) (184000 92%) 4.0540\n",
      "76m 22s (- 6m 24s) (184500 92%) 4.1492\n",
      "76m 33s (- 6m 12s) (185000 92%) 4.1357\n",
      "76m 45s (- 5m 59s) (185500 92%) 4.2343\n",
      "76m 56s (- 5m 47s) (186000 93%) 3.9539\n",
      "77m 5s (- 5m 34s) (186500 93%) 4.0707\n",
      "77m 16s (- 5m 22s) (187000 93%) 4.2293\n",
      "77m 26s (- 5m 9s) (187500 93%) 4.2486\n",
      "77m 36s (- 4m 57s) (188000 94%) 4.0008\n",
      "77m 48s (- 4m 44s) (188500 94%) 4.0230\n",
      "77m 59s (- 4m 32s) (189000 94%) 4.1235\n",
      "78m 10s (- 4m 19s) (189500 94%) 4.1418\n",
      "78m 21s (- 4m 7s) (190000 95%) 4.1193\n",
      "78m 32s (- 3m 55s) (190500 95%) 4.0238\n",
      "78m 42s (- 3m 42s) (191000 95%) 4.1244\n",
      "78m 52s (- 3m 30s) (191500 95%) 4.0709\n",
      "79m 2s (- 3m 17s) (192000 96%) 4.1215\n",
      "79m 13s (- 3m 5s) (192500 96%) 4.0401\n",
      "79m 24s (- 2m 52s) (193000 96%) 4.2140\n",
      "79m 36s (- 2m 40s) (193500 96%) 4.0049\n",
      "79m 47s (- 2m 28s) (194000 97%) 4.1489\n",
      "79m 58s (- 2m 15s) (194500 97%) 4.1006\n",
      "80m 9s (- 2m 3s) (195000 97%) 4.1299\n",
      "80m 19s (- 1m 50s) (195500 97%) 4.0982\n",
      "80m 29s (- 1m 38s) (196000 98%) 4.0181\n",
      "80m 39s (- 1m 26s) (196500 98%) 4.0146\n",
      "80m 51s (- 1m 13s) (197000 98%) 3.8995\n",
      "81m 2s (- 1m 1s) (197500 98%) 4.0374\n",
      "81m 14s (- 0m 49s) (198000 99%) 4.1154\n",
      "81m 25s (- 0m 36s) (198500 99%) 3.9248\n",
      "81m 36s (- 0m 24s) (199000 99%) 4.0091\n",
      "81m 46s (- 0m 12s) (199500 99%) 4.0519\n",
      "81m 56s (- 0m 0s) (200000 100%) 4.2223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 200000, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), \"./models/encoder1_rus_conv_hs_128_unnorm\")\n",
    "torch.save(attn_decoder1.state_dict(), \"./models/attn_decoder1_rus_conv_hs_128_unnorm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> марина, ты здесь?\n",
      "= марина!\n",
      "< здесь. <EOS>\n",
      "\n",
      "> можно вопрос?\n",
      "= да.\n",
      "< да. <EOS>\n",
      "\n",
      "> захватили его?\n",
      "= захватили  его.\n",
      "< нет. <EOS>\n",
      "\n",
      "> он не предаст.\n",
      "= почему вы знаете?\n",
      "< почему это? <EOS>\n",
      "\n",
      "> гитлер.\n",
      "= а еще?\n",
      "< за что? <EOS>\n",
      "\n",
      "> очень хорошо.\n",
      "= и очень рискованно.\n",
      "< как <EOS>\n",
      "\n",
      "> о чем вы думаете?\n",
      "= что сеичас убьюсь.\n",
      "< за <EOS>\n",
      "\n",
      "> волчии сновидец... ты здесь?\n",
      "= входи.\n",
      "< здесь. <EOS>\n",
      "\n",
      "> кто там?\n",
      "= это я, капитан.\n",
      "< это <EOS>\n",
      "\n",
      "> и что же?\n",
      "= она мертва.\n",
      "< я <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я, <EOS>\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"кто\"\n",
    "output_words, attentions = evaluate(encoder1, attn_decoder1, input_sentence)\n",
    "output_sentence = ' '.join(output_words)\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "---------------------\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix, with the columns being input steps and rows being\n",
    "output steps:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17ed17278>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 672x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"дела?\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes\n",
    "and labels:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'елка'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-293919be6872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"елка .\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"воу .\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-293919be6872>\u001b[0m in \u001b[0;36mevaluateAndShowAttention\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     output_words, attentions = evaluate(\n\u001b[0;32m---> 22\u001b[0;31m         encoder1, attn_decoder1, input_sentence)\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-881d27a56888>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bb69e91c7422>\u001b[0m in \u001b[0;36mtensorFromSentence\u001b[0;34m(lang, sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEOS_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bb69e91c7422>\u001b[0m in \u001b[0;36mindexesFromSentence\u001b[0;34m(lang, sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bb69e91c7422>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'елка'"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"елка .\")\n",
    "\n",
    "evaluateAndShowAttention(\"воу .\")\n",
    "\n",
    "evaluateAndShowAttention(\"привет .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Try with a different dataset\n",
    "\n",
    "   -  Another language pair\n",
    "   -  Human → Machine (e.g. IOT commands)\n",
    "   -  Chat → Response\n",
    "   -  Question → Answer\n",
    "\n",
    "-  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
    "   GloVe\n",
    "-  Try with more layers, more hidden units, and more sentences. Compare\n",
    "   the training time and results.\n",
    "-  If you use a translation file where pairs have two of the same phrase\n",
    "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
    "   this:\n",
    "\n",
    "   -  Train as an autoencoder\n",
    "   -  Save only the Encoder network\n",
    "   -  Train a new Decoder for translation from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
